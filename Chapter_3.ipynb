{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62215520",
   "metadata": {},
   "source": [
    "# CHAPTER 3 ETHICS AND PRIVACY Homework and Quiz #\n",
    "### By Aaditya Geddam PUBH 1142 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5784f8a",
   "metadata": {},
   "source": [
    "## TASK 1: Misuse of Health Data ##\n",
    "\n",
    "**Example: GoodRx Sharing Health Data with Advertisers (2023)**\n",
    "\n",
    "In February 2023, the Federal Trade Commission took enforcement action against GoodRx, a popular prescription drug discount app used by millions of Americans, for sharing users' sensitive health information with advertising platforms without their knowledge or consent. GoodRx had collected detailed information about users' medications and health conditions, including data about medications for mental health issues, HIV, erectile dysfunction, and other sensitive conditions. Despite repeatedly promising users that their health data would be kept private and even displaying a \"HIPAA Secure: Patient Data Protected\" seal on their website, GoodRx shared this information with Facebook, Google, and other advertising platforms to target users with ads. The company used tracking pixels and other technologies to send personally identifiable information along with medication and health data to these third parties, allowing them to build detailed profiles of users' health conditions.\n",
    "\n",
    "The ethical issues here are pretty clear-cut and disturbing. First, there's the massive breach of trust—GoodRx explicitly told users their data would be protected and wouldn't be shared, but then did exactly the opposite for profit. The company even falsely implied it was HIPAA-compliant when it wasn't subject to HIPAA regulations at all, which is straight-up deceptive. Second, there's the issue of informed consent, or really the complete lack of it. Users had no idea their sensitive health information was being shared with advertisers, and they certainly didn't consent to Facebook and Google knowing what medications they were taking. Third, the nature of the data itself makes this particularly harmful—people searching for discounts on HIV medications or antidepressants are often in vulnerable situations, and having that information weaponized for targeted advertising is exploitative. The shared data could potentially be used for discrimination in employment, insurance, or other contexts if it fell into the wrong hands.\n",
    "\n",
    "To prevent this from happening, several measures should be implemented. First, there needs to be stricter enforcement of truth-in-advertising rules for health apps—if a company claims to be \"HIPAA Secure,\" there should be serious penalties for misleading users like that. The FTC's $1.5 million fine honestly seems pretty small compared to the profits GoodRx made from this practice. Second, opt-in consent should be mandatory for any sharing of health data with third parties, period. The default should be that data stays private unless users explicitly agree otherwise, and that agreement should explain in plain language exactly who's getting the data and what they'll do with it. Third, there should be technical audits required before health apps can go to market, checking for tracking pixels and other data-sharing tools that users can't see. Finally, we need to expand regulations like HIPAA to cover more digital health companies—right now there's a huge loophole where apps and websites can collect health data without the same protections that hospitals and doctors have to follow. The GoodRx case led to a broader FTC enforcement push, with similar actions against BetterHelp and other health apps, which shows this was a widespread industry problem that needed regulatory attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823136b",
   "metadata": {},
   "source": [
    "## TASK 2: Breach in Health Data Security ##\n",
    "\n",
    "**Example: Change Healthcare Cyberattack (2024)**\n",
    "\n",
    "One of the most devastating recent health data breaches occurred in February 2024 when Change Healthcare, a major health technology company owned by UnitedHealth Group, was hit by a massive ransomware attack. This breach affected approximately 190 million individuals, making it one of the largest healthcare data breaches in U.S. history. The attackers gained access through compromised credentials and were able to infiltrate systems that process about one-third of all patient records in the United States. The stolen data included names, addresses, dates of birth, Social Security numbers, health insurance information, and medical records.\n",
    "\n",
    "The consequences of this breach were massive and far-reaching. Beyond the obvious privacy violations for millions of Americans, the attack caused significant operational disruptions across the healthcare system. Hospitals and pharmacies couldn't process insurance claims or prescriptions for weeks, which meant patients faced delays in getting medications and providers struggled to get paid. The financial impact was enormous too—UnitedHealth Group reported over $2.3 billion in direct costs related to the breach, and many healthcare providers experienced cash flow problems. There's also the ongoing risk of identity theft and fraud for the affected individuals, since sensitive personal and medical information is now potentially in the hands of cybercriminals.\n",
    "\n",
    "Several measures could have prevented or mitigated this breach. First, Change Healthcare should have implemented multi-factor authentication (MFA) on all systems, which would have made it much harder for attackers to gain access even with stolen credentials. Second, better network segmentation could have limited the attackers' ability to move laterally through the system once they got in. Third, more frequent security audits and penetration testing might have identified vulnerabilities before hackers could exploit them. Finally, having better backup and disaster recovery systems in place could have reduced the operational disruption even if the attack couldn't be completely prevented. This breach really highlights how interconnected the healthcare system is and how a single point of failure can have ripple effects across the entire industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c2d1e",
   "metadata": {},
   "source": [
    "## TASK 3: Bias in Health Data ##\n",
    "Example: Algorithmic Bias in Healthcare Risk Prediction\n",
    "\n",
    "A significant example of bias in health data involves algorithms used by healthcare systems to predict which patients need extra medical care. A 2019 study published in Science found that a widely used commercial algorithm that affected millions of patients was systematically discriminating against Black patients. The algorithm was designed to identify high-risk patients who would benefit from extra care management programs, but it used healthcare costs as a proxy for healthcare needs. This seemed logical on the surface, but it created a major bias because Black patients typically have less money spent on their care compared to White patients with the same level of illness, largely due to systemic inequalities in healthcare access and quality.\n",
    "\n",
    "The consequences of this bias were pretty disturbing. The algorithm effectively screened out Black patients who were just as sick as White patients, denying them access to programs that could have improved their health outcomes. Specifically, the study found that the algorithm reduced the number of Black patients identified for extra care by more than half compared to what would have happened with an unbiased system. This perpetuated existing health disparities by giving preferential treatment to White patients and reinforcing the cycle where Black patients receive less care, which leads to worse outcomes, which then gets built into future algorithms. It's a perfect example of how bias can get encoded into supposedly \"objective\" systems and actually make existing inequalities worse.\n",
    "\n",
    "Prevention of this type of bias requires several approaches. First, algorithm developers need to use more diverse training data that includes adequate representation of different racial and ethnic groups. Second, instead of using healthcare spending as a proxy for medical need, algorithms should use more direct measures of health status like the number of chronic conditions or specific health markers. Third, there needs to be mandatory bias testing before algorithms are deployed in clinical settings, specifically looking at how they perform across different demographic groups. Finally, there should be ongoing monitoring after implementation to catch bias that might emerge over time as the algorithm learns from new data. The researchers who discovered this bias actually worked with the company to develop a less biased version of the algorithm, showing that these problems can be fixed once they're identified and taken seriously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e2e10",
   "metadata": {},
   "source": [
    "## TASK 4: Anonymization of Health Data ##\n",
    "Example: Differential Privacy in COVID-19 Surveillance\n",
    "\n",
    "During the COVID-19 pandemic, public health agencies needed to share detailed data about case counts and transmission patterns while protecting individual privacy. One innovative technique used was differential privacy, which was implemented in several COVID-19 data dashboards and contact tracing systems. A notable example comes from Australia's public health surveillance system, which used differential privacy to allow researchers and the public to query COVID-19 case data across multiple variables like location, time, and demographics without compromising individual confidentiality. The technique works by adding carefully calibrated statistical noise to query results, so if someone tries to figure out whether a specific person has COVID-19 by comparing queries, the added randomness makes it mathematically impossible to be certain.\n",
    "​\n",
    "\n",
    "The differential privacy technique used in COVID-19 surveillance is pretty clever because it provides strong mathematical guarantees about privacy protection. Every time someone queries the database—for example, asking \"How many COVID-19 cases were there in this neighborhood last week?\"—the system adds a small amount of random noise to the answer. The key is that this noise is calibrated carefully enough that the overall patterns and trends remain accurate for public health decision-making, but any individual person's data is protected. The system also tracks a \"privacy budget\" that limits how many queries can be made, because if you make unlimited queries with different parameters, you could potentially narrow down individual cases even with the noise. Another major implementation was the Apple/Google Exposure Notification System, which used differential privacy principles alongside other privacy-preserving techniques to enable COVID-19 contact tracing without revealing individuals' locations or identities.\n",
    "\n",
    "This approach had several advantages over traditional anonymization methods. Unlike simple data masking or k-anonymity, differential privacy provides mathematical proof of privacy protection rather than just hoping that combinations of data points won't lead to re-identification. It also allowed for much more granular data sharing—public health officials could release data at finer geographic and temporal resolutions than they could with conventional methods, which was crucial for targeted pandemic response efforts. However, there were trade-offs involved. The added noise means that very small counts might be less accurate, which could be problematic for tracking outbreaks in small communities. The U.S. Census Bureau's implementation of differential privacy for the 2020 Census actually raised concerns among public health researchers because the errors introduced were larger for small populations and racial minorities, potentially affecting the accuracy of health equity research. Despite these challenges, differential privacy represents a significant advancement in balancing data utility with privacy protection, especially for time-sensitive public health emergencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d6523",
   "metadata": {},
   "source": [
    "## TASK 5: George Washington University Office of Human Research ##\n",
    "The George Washington University Office of Human Research (OHR) is basically the main office that oversees all research involving human subjects at GW, making sure everything is done ethically and that participants are protected. They work with the Institutional Review Boards to review research proposals through different processes depending on the risk level—some studies get full board review if they're higher risk, while lower-risk studies can go through expedited review or even get exempted from full oversight. Everything is done electronically now through the GW iRIS system, which makes it way easier for researchers to submit applications and track their approvals without dealing with a bunch of paperwork. The OHR also handles \"reliance agreements\" with other universities' IRBs for multi-site studies, so you don't have every single institution reviewing the exact same protocol separately. Overall, they're kind of the ethical gatekeepers for human research at GW—they make sure researchers follow proper procedures, protect participants' rights, and maintain research integrity, which is honestly pretty important when you think about all the ways research could go wrong without proper oversight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504064d",
   "metadata": {},
   "source": [
    "# QUIZ ANSWERS # \n",
    "1. What is the primary goal of data anonymization?\n",
    "\n",
    "To protect private or sensitive information by erasing or encrypting identifiers that connect an individual to stored data, making it impossible or extremely difficult to identify individuals while keeping the data useful for analysis.\n",
    "\n",
    "2. What is the Havasupai Tribe case about?\n",
    "\n",
    "Researchers from Arizona State University collected blood samples from the Havasupai Tribe in the 1990s for diabetes research but used them for additional unauthorized studies on schizophrenia and geographic origins without consent, violating the tribe's rights and cultural beliefs.\n",
    "\n",
    "3. What was the largest health data breach in history?\n",
    "\n",
    "The Anthem Inc. breach in 2015, which affected 78.8 million individuals (though the 2024 Change Healthcare breach affected approximately 190 million people and may now be considered larger).\n",
    "\n",
    "4. What is the pulse oximeter controversy about?\n",
    "\n",
    " Pulse oximeters were found to be nearly three times more likely to miss low blood oxygen levels in Black patients compared to White patients because their algorithms were based primarily on data from light-skinned individuals.\n",
    "\n",
    "5. What is the role of an Institutional Review Board (IRB) in research?\n",
    "\n",
    "To review and monitor research involving human subjects, ensuring it is conducted ethically and that participants' rights and welfare are adequately protected, including approving studies before they begin and providing ongoing oversight.\n",
    "\n",
    "6. What is the difference between spreadsheet software and database software for health data management?\n",
    "\n",
    "Spreadsheet software like Excel is basically good for smaller projects where you're working on your own and need to organize data in a simple table format with rows and columns. It's easy to use and great for basic analysis and making charts, but it starts to fall apart when you have tons of data or need multiple people working on the same file at once. Database software like SQL or REDCap is way more powerful for handling large amounts of health data, as it can manage multiple connected tables, let several users access it simultaneously without messing things up, and has built-in rules to make sure the data stays accurate and consistent. Databases also have better security features which is super important for health data since you need to follow HIPAA regulations and protect patient privacy.\n",
    "\n",
    "\n",
    "7. What is a conflict of interest in public health research?\n",
    "\n",
    "A situation where a person or organization has a secondary interest (financial, personal, political, academic, or organizational) that could unduly influence or bias the conduct, findings, or reporting of research.\n",
    "\n",
    "8. What is informed consent in the context of health data collection?\n",
    "\n",
    " The process of providing individuals with sufficient information about the purpose of data collection, how data will be used, who will have access, potential risks and benefits, and their right to withdraw consent, allowing them to make an educated decision about participation.\n",
    "\n",
    "9. What is data masking in the context of de-identification of health data?\n",
    "\n",
    "A technique that involves replacing identifiable information with random characters or removing it entirely (ex. replacing \"John Doe\" with \"XXXX XXXX\").\n",
    "\n",
    "10. What is the principle of k-anonymity in the context of de-identification of health data?\n",
    "\n",
    "A technique that groups data so each group contains at least k records, making it difficult to single out individuals (ex. grouping patients into income brackets with at least k patients in each bracket).\n",
    "\n",
    "11. What is the main concern about conflicts of interest in public health research?\n",
    "\n",
    "They can compromise research integrity by introducing bias, affecting the validity of findings, eroding public trust, and potentially influencing health policies or clinical practices based on flawed evidence.\n",
    "\n",
    "12. What is the first step in managing conflicts of interest in public health research?\n",
    "\n",
    "Transparency through disclosure researchers should disclose any potential conflicts of interest at the outset of a study and update these disclosures throughout the research process.\n",
    "\n",
    "13. What is the role of independent oversight in managing conflicts of interest in public health research?\n",
    "\n",
    "To provide an additional layer of security by having a separate committee review research proposals for potential conflicts, monitor ongoing research, or review findings before publication.\n",
    "\n",
    "14. What is one strategy for mitigating conflicts of interest in public health research?\n",
    "\n",
    "Avoidance, divestment, management plans, public disclosure, institutional policies, or education and training \n",
    "\n",
    "15. What is the potential impact of not properly managing conflicts of interest in public health research?\n",
    "\n",
    "Biased research results, loss of public trust, damage to research integrity, and potentially harmful influences on health policies or clinical practices that affect patient care."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
